{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9d626e8e67ee8e",
   "metadata": {
    "collapsed": false,
    "id": "5a9d626e8e67ee8e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Introduction to score based generative modelling.\n",
    "\n",
    "In this notebook, we provide an introduction to generative modelling via the **score** of a distribution. If $q_d$ is the density of the distribution of interest, the score of $q_d$ is $ \\nabla \\log q_d(x)\\,. $\n",
    "\n",
    "**Score based generative modelling** consists of estimating the score of a distribution of interest from a set of samples $\\mathcal{D} = \\{X_1, \\cdots, X_n\\}$. Therefore, we search, for a given parametric family $s_{\\theta}(x)$ (i.e. a given network structure), the parameter\n",
    "\n",
    "$$\\theta \\in \\operatorname{argmin}_{\\theta} \\mathbb{E}_{X \\sim q_d}[\\|s_{\\theta}(X) - \\nabla \\log q_d(X)\\|^2] \\,. $$\n",
    "\n",
    "Of course, it is not possible to directly solve the optimization problem presented above, since we do not know the score of the distribution.\n",
    "In this notebook, we will consider three approaches to learning the score of a distribution $p$ without knowing the density (or the score):\n",
    "\n",
    "* [1] Hyvärinen, A. (2005). Estimation of Non-Normalized Statistical Models by Score Matching. Journal of Machine Learning Research, 6(24), 695–709. Retrieved from http://jmlr.org/papers/v6/hyvarinen05a.html\n",
    "\n",
    "* [2] P. Vincent, \"A Connection Between Score Matching and Denoising Autoencoders,\" in Neural Computation, vol. 23, no. 7, pp. 1661-1674, July 2011, doi: 10.1162/NECO_a_00142.\n",
    "\n",
    "* [3] Song, Y., & Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:00:54.915717732Z",
     "start_time": "2024-02-25T00:00:49.440498345Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "9b4f451c-792d-4aa4-ddff-7bab4fce8d89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f2ccddc7dd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from jax import numpy as jnp, grad, random, vmap, value_and_grad, jit, jvp, jacfwd, devices\n",
    "from jax.tree_util import Partial\n",
    "from jax.lax import scan\n",
    "import numpyro.distributions as dist\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import math\n",
    "import optax\n",
    "from tqdm.notebook import tqdm\n",
    "import orbax.checkpoint\n",
    "\n",
    "#Defining master key for jax\n",
    "KEY = random.key(0)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['animation.embed_limit'] = 2**128\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8ab384da602442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:00:55.423974408Z",
     "start_time": "2024-02-25T00:00:53.938469011Z"
    },
    "id": "3b8ab384da602442"
   },
   "outputs": [],
   "source": [
    "# Defining Noised version of original gaussian mixture. q_d corresponds to p_t(0).\n",
    "def p_t(std_t):\n",
    "    means = jnp.meshgrid(jnp.arange(-10., 15., 10), jnp.arange(-10., 15., 10))\n",
    "    means = jnp.stack([m.flatten() for m in means], axis=1)\n",
    "    covs = jnp.repeat(jnp.eye(2)[None]*(.1 + std_t**2), axis=0, repeats=means.shape[0])\n",
    "    weights = random.uniform(random.key(42), (len(means),))\n",
    "    return dist.MixtureSameFamily(component_distribution=dist.MultivariateNormal(means,\n",
    "                                                                                 covariance_matrix=covs),\n",
    "                                  mixing_distribution=dist.Categorical(weights))\n",
    "\n",
    "# Helper function to get the score values on a mesh\n",
    "def get_mesh_score(score_fn):\n",
    "    X, Y = jnp.meshgrid(jnp.linspace(-15, 15), jnp.linspace(-15, 15))\n",
    "    xs = jnp.stack([X.flatten(), Y.flatten()], axis=1)\n",
    "    scores = score_fn(xs)\n",
    "    return X, Y, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782b3e4-fd88-4490-a7f5-29c128618bf7",
   "metadata": {
    "id": "1782b3e4-fd88-4490-a7f5-29c128618bf7"
   },
   "source": [
    "## Unadjusted Langevin Algorithm\n",
    "\n",
    "In this notebook, we will sample from a learned score model using the Unadjusted Langevin Algorithm (ULA). We refer to the following papers for a presentation of the algorithm and it's performance.\n",
    "\n",
    "* [4] Roberts, G. O., & Tweedie, R. L. (1996). Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, 341-363.\n",
    "* [5] Durmus, A., Majewski, S., & Miasojedow, B. (2019). Analysis of Langevin Monte Carlo via convex optimization. The Journal of Machine Learning Research, 20(1), 2666-2711."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51aacc96cd032253",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:00:55.644257785Z",
     "start_time": "2024-02-25T00:00:53.951672057Z"
    },
    "id": "51aacc96cd032253"
   },
   "outputs": [],
   "source": [
    "# ULA kernel\n",
    "def ula(x, key, score_fun, learning_rate):\n",
    "    noise = random.normal(key=key, shape=x.shape)\n",
    "    return x + learning_rate * score_fun(x) + ((2*learning_rate)**.5)*noise\n",
    "\n",
    "# Function that is basically a for loop over ULA step but return every sample.\n",
    "# To better understand the syntax of Jax's scan function go to: https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html\n",
    "def several_steps_ula(init, key, score_fun, learning_rate, n_steps):\n",
    "    \n",
    "    def my_ula(x, key):\n",
    "        pred = ula(x, key, score_fun, learning_rate)\n",
    "        return pred, pred\n",
    "    return scan(f=my_ula,\n",
    "                init=init,\n",
    "                xs=random.split(key, n_steps))[-1]\n",
    "\n",
    "# This is a helper function to allow for multi step ula.\n",
    "def multiple_chain_ula(init, keys, n_steps, score_fun, learning_rate):\n",
    "    return vmap(Partial(several_steps_ula, n_steps=n_steps, learning_rate=learning_rate, score_fun=score_fun))(init, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5d8327-7b88-481d-bff2-79bbeff267b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:00:57.203121099Z",
     "start_time": "2024-02-25T00:00:54.235847384Z"
    },
    "id": "0e5d8327-7b88-481d-bff2-79bbeff267b8"
   },
   "outputs": [],
   "source": [
    "# This cell initializes some generic global variables used throughout the notebook \n",
    "LANGEVIN_LR=1e-2\n",
    "N_EPOCHS=3_000\n",
    "BATCH_SIZE=2048\n",
    "KEY, subkey = random.split(KEY, 2)\n",
    "dataset = p_t(0).sample(subkey, (10_000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe157672-7e50-4c4f-8b2d-e7e978b17afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:00:59.393615469Z",
     "start_time": "2024-02-25T00:00:55.612280348Z"
    },
    "id": "fe157672-7e50-4c4f-8b2d-e7e978b17afc"
   },
   "outputs": [],
   "source": [
    "# Performs ULA using the real score\n",
    "KEY, subkey_init, subkey_keys = random.split(KEY, 3)\n",
    "samples_ula_0 = multiple_chain_ula(random.normal(subkey_init, shape=(20, 2))*10,\n",
    "                                   random.split(subkey_keys, 20),\n",
    "                                   learning_rate=LANGEVIN_LR,\n",
    "                                   score_fun=jit(grad(lambda x: p_t(0).log_prob(x))),\n",
    "                                   n_steps=1_000)\n",
    "\n",
    "X, Y, scores = get_mesh_score(vmap(grad(p_t(0).log_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe9cf8-6c65-41a8-829a-7c15c6ffa7d4",
   "metadata": {
    "id": "79fe9cf8-6c65-41a8-829a-7c15c6ffa7d4"
   },
   "source": [
    "We consider $p$ to be a $2$-dimensional Gaussian mixture distribution.\n",
    "Below we visualize the result of running ULA with the real score of $q_d$. We initialize the ULA chain with samples from $\\mathcal{N}(0, 100 \\operatorname{I})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8aed6-54fe-4b76-823a-e032ddf33295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:01:18.951425440Z",
     "start_time": "2024-02-25T00:00:59.139556864Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "id": "66e8aed6-54fe-4b76-823a-e032ddf33295",
    "outputId": "af23db7d-9431-46e6-f459-f0160182db7e"
   },
   "outputs": [],
   "source": [
    "# Animate ULA with the real score\n",
    "KEY, subkey = random.split(KEY, 2)\n",
    "samples = p_t(0).sample(subkey, (1000,))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "for ax in axes:\n",
    "  ax.scatter(*samples[:1000].T, color='blue')\n",
    "  ax.quiver(X, Y, scores[:, 0], scores[:, 1], color='black')\n",
    "  ax.set_xlim(-15, 15)\n",
    "  ax.set_ylim(-15, 15)\n",
    "traj_artists, points_artists = [], []\n",
    "for i, traj in enumerate(samples_ula_0[:, :1]):\n",
    "    point_artist = axes[1].scatter(*traj[-1], color=plt.get_cmap(\"rainbow\")(i / (samples_ula_0.shape[0]-1)))\n",
    "    traj_artist, = axes[0].plot(*traj.T, color=plt.get_cmap(\"rainbow\")(i / (samples_ula_0.shape[0]-1)))\n",
    "    traj_artists.append(traj_artist)\n",
    "    points_artists.append(point_artist)\n",
    "artists = traj_artists + points_artists\n",
    "\n",
    "def init():\n",
    "\n",
    "  return artists\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    n_artists = len(artists) //2\n",
    "    for i, traj in enumerate(samples_ula_0[:, :frame + 1]):\n",
    "        artists[i].set_data(traj[:, 0], traj[:, 1])\n",
    "        artists[i + n_artists].set_offsets(traj[-1])\n",
    "    return artists\n",
    "\n",
    "matplotlib.animation.FuncAnimation(fig,  update, frames=jnp.arange(0, 25, 1).tolist(),\n",
    "                    init_func=init, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db118afdc8f7bce",
   "metadata": {
    "collapsed": false,
    "id": "6db118afdc8f7bce",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Hyvärinen Approach [1]\n",
    "\n",
    "We start by the Loss presented in [1]. By integration by parts, it is possible to show that the score matching objective is equivalent to:\n",
    "\n",
    "$$ \\theta \\in \\operatorname{argmin}_{\\theta} \\mathbb{E}_{X \\sim q_d}[\\nabla \\cdot s_{\\theta}(X) + \\frac{1}{2} \\|s_{\\theta}(X)\\|^2] \\,. $$\n",
    "\n",
    "We use a simple MLP (multi-layer perceptron) network to model the score $s_\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b59e28f2defc35",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-25T00:02:02.770272472Z"
    },
    "id": "d8b59e28f2defc35"
   },
   "outputs": [],
   "source": [
    "# This function defines a general training function used throught the paper\n",
    "def train(dataset,\n",
    "          learning_rate,\n",
    "          net_params,\n",
    "          loss_fn,\n",
    "          key,\n",
    "          n_epochs,\n",
    "          batch_size = None,\n",
    "         update_tqdm_period=100):\n",
    "    tx = optax.adam(learning_rate=learning_rate)\n",
    "    opt_state = tx.init(net_params)\n",
    "\n",
    "    # define the heavy lifting function of the training loop\n",
    "    def loop_fn_(sb_k, net_params, opt_state):\n",
    "        sb_k_loss, sb_k_batch = random.split(sb_k, 2)\n",
    "        if batch_size:\n",
    "            batch_data = dataset[random.randint(key=sb_k_batch, maxval=dataset.shape[0], minval=0, shape=(batch_size,))]\n",
    "        else:\n",
    "            batch_data = dataset\n",
    "        loss_val, grad = loss_fn(net_params, batch_data, sb_k_loss)\n",
    "        updates, opt_state = tx.update(grad, opt_state)\n",
    "\n",
    "        net_params = optax.apply_updates(net_params, updates)\n",
    "        return loss_val, net_params, opt_state\n",
    "    # Mark it to be just-in-time compiled\n",
    "    loop_fn = jit(loop_fn_)\n",
    "\n",
    "\n",
    "    pbar = tqdm(enumerate(random.split(key, n_epochs)))\n",
    "    losses = []\n",
    "    for i, sb_k in pbar:\n",
    "        # Do stuff\n",
    "        loss_val, net_params, opt_state = loop_fn(sb_k, net_params, opt_state)\n",
    "\n",
    "        # Plot stuff\n",
    "        if i % update_tqdm_period == 0:\n",
    "            pbar.set_description(f\"Loss: {loss_val:.2E}\")\n",
    "        losses.append(loss_val)\n",
    "    return net_params, losses\n",
    "\n",
    "\n",
    "# Loss from Hyvarinen paper.\n",
    "def make_hyvarinen_loss(net, n_eps_hutch=100, data_dim=2, use_hutching_trick=False):\n",
    "    if use_hutching_trick:\n",
    "        # To be completed\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        def loss_fn(params, data, key):\n",
    "            def score_twice(data):\n",
    "                score = net.apply(params, data)\n",
    "                return score, score\n",
    "            jac, score = vmap(jacfwd(score_twice, has_aux=True))(data)\n",
    "\n",
    "            div = jnp.diagonal(jac, axis1=-2, axis2=-1).sum(axis=-1)\n",
    "            score_norm = (score**2).sum(axis=-1)\n",
    "\n",
    "            return (div + .5 * score_norm).mean()\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a7964f7562360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:02:03.349987828Z",
     "start_time": "2024-02-25T00:02:02.771956468Z"
    },
    "id": "9a1a7964f7562360"
   },
   "outputs": [],
   "source": [
    "# Simple Neural Net from Flax\n",
    "class MLP(nn.Module):              \n",
    "  out_dims: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "      x = nn.Dense(128)(x)\n",
    "      x = nn.LayerNorm()(x)\n",
    "      x = nn.leaky_relu(x)\n",
    "      x = nn.Dense(512)(x)\n",
    "      x = nn.LayerNorm()(x)\n",
    "      x = nn.leaky_relu(x)\n",
    "      x = nn.Dense(256)(x)\n",
    "      x = nn.LayerNorm()(x)  \n",
    "      x = nn.leaky_relu(x)\n",
    "      x = nn.Dense(self.out_dims)(x)\n",
    "      return nn.leaky_relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5231a4992abce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:02:03.489835125Z",
     "start_time": "2024-02-25T00:02:02.778917556Z"
    },
    "id": "bbd5231a4992abce"
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "model = MLP(out_dims=2)\n",
    "x = jnp.empty((1, 2))\n",
    "net_params = model.init(random.key(42), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80982b538ebeceaf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-25T00:02:02.783477572Z"
    },
    "id": "80982b538ebeceaf",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "KEY, subkey = random.split(KEY, 2)\n",
    "net_params, losses = train(\n",
    "    dataset=dataset,\n",
    "    learning_rate=1e-4,\n",
    "    net_params=net_params,\n",
    "    loss_fn=value_and_grad(make_hyvarinen_loss(model)),\n",
    "    key=subkey,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d1b27-00b4-4e15-80cf-a6f2c17675a2",
   "metadata": {
    "id": "072d1b27-00b4-4e15-80cf-a6f2c17675a2",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# PLotting the loss\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(jnp.stack(losses))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cc3d7-2b9a-4a7e-a8fd-a9a6afaefa19",
   "metadata": {
    "id": "782cc3d7-2b9a-4a7e-a8fd-a9a6afaefa19"
   },
   "source": [
    "### Sampling with Hyvarinen Model:\n",
    "\n",
    "We now focus on sampling with the Hyvarinen model musing ULA with the same parameterization as for the case of perfect score, and the same starting distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf2889-2960-47e5-bdc4-e5ddeb78e308",
   "metadata": {
    "id": "bcaf2889-2960-47e5-bdc4-e5ddeb78e308",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Sampling with ULA\n",
    "KEY, subkey_init, subkey_keys = random.split(KEY, 3)\n",
    "samples_ula_0 = multiple_chain_ula(random.normal(subkey_init, shape=(20, 2))*10,\n",
    "                                   random.split(subkey_keys, 20),\n",
    "                                   learning_rate=LANGEVIN_LR,\n",
    "                                   score_fun=jit(lambda x: model.apply(net_params, x)),\n",
    "                                   n_steps=1_000)\n",
    "X, Y, scores = get_mesh_score(lambda xs: vmap(model.apply, in_axes=(None, 0))(net_params, xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf5a59-96d6-4359-b019-bfc6047705e3",
   "metadata": {
    "id": "d7cf5a59-96d6-4359-b019-bfc6047705e3",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Animating with ULA\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "traj_artists, points_artists = [], []\n",
    "for i, traj in enumerate(samples_ula_0[:, :1]):\n",
    "    point_artist = axes[1].scatter(*traj[-1], color=plt.get_cmap(\"rainbow\")(i / (samples_ula_0.shape[0]-1)))\n",
    "    traj_artist, = axes[0].plot(*traj.T, color=plt.get_cmap(\"rainbow\")(i / (samples_ula_0.shape[0]-1)))\n",
    "    traj_artists.append(traj_artist)\n",
    "    points_artists.append(point_artist)\n",
    "artists = traj_artists + points_artists\n",
    "\n",
    "def init():\n",
    "  for ax in axes:\n",
    "      ax.scatter(*dataset[:1000].T, color='blue')\n",
    "      ax.quiver(X, Y, scores[:, 0], scores[:, 1], color='black')\n",
    "      ax.set_xlim(-15, 15)\n",
    "      ax.set_ylim(-15, 15)\n",
    "  return artists\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    n_artists = len(artists) //2\n",
    "    for i, traj in enumerate(samples_ula_0[:, :frame + 1]):\n",
    "        artists[i].set_data(traj[:, 0], traj[:, 1])\n",
    "        artists[i + n_artists].set_offsets(traj[-1])\n",
    "    return artists\n",
    "\n",
    "matplotlib.animation.FuncAnimation(fig,  update, frames=jnp.arange(0, 1_000, 50).tolist(),\n",
    "                    init_func=init, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd0672d74e689a",
   "metadata": {
    "collapsed": false,
    "id": "67fd0672d74e689a",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Denoising Loss [2]:\n",
    "\n",
    "In [2], the authors propose learning the score of a noised version of $q_d$, that we denote by $q_t$. Formally,\n",
    "$$q_t(x) = \\mathbb{E}_{X \\sim q_d}[q_{t|0}(x | X)] \\, ,$$\n",
    "where $q_{t|0}(x | x_0) = \\mathcal{N}(x; x_{0}, \\upsilon_t^2 \\operatorname{I})$ with $\\upsilon_t > 0$.\n",
    "\n",
    "In this case, estimating the score of $q_t$ via the score mathing objective is equivalent to\n",
    "$$ \\operatorname{argmin}_{\\theta}\\mathbb{E}_{X \\sim q_d, \\epsilon \\sim \\mathcal{N}(0, \\operatorname{I})} [\\|s_{\\theta}(X + \\upsilon_t \\epsilon) - \\nabla \\log q_{t|0}(X + \\upsilon_t \\epsilon|X)\\|^2]  = \\operatorname{argmin}_{\\theta}\\mathbb{E}_{X \\sim q_d, \\epsilon \\sim \\mathcal{N}(0, \\operatorname{I})}[\\|s_{\\theta}(X + \\upsilon_t \\epsilon) + \\upsilon_t^{-1}\\epsilon \\|^2]\\, . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10fc86db-b957-4650-a8d9-5b7430d11cf2",
   "metadata": {
    "id": "10fc86db-b957-4650-a8d9-5b7430d11cf2",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Function that makes the denoising loss function\n",
    "def make_denoising_loss(net, std=1e-2, data_dim=2):\n",
    "    def loss_fn(params, data, key):\n",
    "        eps = random.normal(key, (data.shape[0], data_dim))\n",
    "        score = net.apply(params, data + std*eps)\n",
    "\n",
    "        loss = ((score + eps/std)**2).sum(axis=-1)\n",
    "\n",
    "        return loss.mean()\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425b307-93f9-41c5-a8d1-0c3e052a7ccf",
   "metadata": {
    "id": "4425b307-93f9-41c5-a8d1-0c3e052a7ccf",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model = MLP(out_dims=2)\n",
    "x = jnp.empty((1, 2))\n",
    "net_params = model.init(random.key(42), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e313215-c9ca-4f3f-a6ef-83587ecea0c7",
   "metadata": {
    "id": "1e313215-c9ca-4f3f-a6ef-83587ecea0c7",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "KEY, subkey = random.split(KEY, 2)\n",
    "net_params, losses = train(\n",
    "    dataset=dataset,\n",
    "    learning_rate=1e-2,\n",
    "    net_params=net_params,\n",
    "    loss_fn=value_and_grad(make_denoising_loss(model, std=1)),\n",
    "    key=subkey,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    update_tqdm_period=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642c638-bdaa-4e9a-a623-3b734c510ec1",
   "metadata": {
    "id": "8642c638-bdaa-4e9a-a623-3b734c510ec1",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(jnp.stack(losses))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce86912-d588-4168-8374-910414109279",
   "metadata": {
    "id": "7ce86912-d588-4168-8374-910414109279"
   },
   "source": [
    "### Sampling from the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712c734-1a54-4c58-a3e7-f8b086234d49",
   "metadata": {
    "id": "9712c734-1a54-4c58-a3e7-f8b086234d49",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "KEY, subkey_init, subkey_keys = random.split(KEY, 3)\n",
    "samples_ula_0 = multiple_chain_ula(random.normal(subkey_init, shape=(20, 2))*10,\n",
    "                                   random.split(subkey_keys, 20),\n",
    "                                   learning_rate=LANGEVIN_LR,\n",
    "                                   score_fun=jit(lambda x: model.apply(net_params, x)),\n",
    "                                   n_steps=1_000)\n",
    "\n",
    "X, Y, scores = get_mesh_score(lambda xs: vmap(model.apply, in_axes=(None, 0))(net_params, xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec7667-9c55-4122-868f-99f97cf0a27d",
   "metadata": {
    "id": "adec7667-9c55-4122-868f-99f97cf0a27d",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.scatter(*dataset[:1000].T, color='blue', alpha=.6)\n",
    "    ax.quiver(X, Y, scores[:, 0], scores[:, 1], color='black')\n",
    "    ax.set_xlim(-15, 15)\n",
    "    ax.set_ylim(-15, 15)\n",
    "\n",
    "traj_artists, points_artists = [], []\n",
    "for i, traj in enumerate(samples_ula_0[:, :1]):\n",
    "    point_artist = axes[1].scatter(*traj[-1], color=plt.get_cmap(\"rainbow\")(i / (samples_ula_0.shape[0]-1)), alpha=.8)\n",
    "    traj_artist, = axes[0].plot(*traj.T, color=plt.get_cmap(\"rainbow\")(i / (samples_ula_0.shape[0]-1)), alpha=.8)\n",
    "    traj_artists.append(traj_artist)\n",
    "    points_artists.append(point_artist)\n",
    "artists = traj_artists + points_artists\n",
    "\n",
    "def init():\n",
    "\n",
    "  return artists\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    n_artists = len(artists) //2\n",
    "    for i, traj in enumerate(samples_ula_0[:, :frame + 1]):\n",
    "        artists[i].set_data(traj[:, 0], traj[:, 1])\n",
    "        artists[i + n_artists].set_offsets(traj[-1])\n",
    "    return artists\n",
    "\n",
    "matplotlib.animation.FuncAnimation(fig,  update, frames=jnp.arange(0, 1_000, 10).tolist(),\n",
    "                    init_func=init, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7a3c8-7f72-4ea0-aacd-af22b9ed3af9",
   "metadata": {
    "id": "dfb7a3c8-7f72-4ea0-aacd-af22b9ed3af9"
   },
   "source": [
    "## Noise Conditional Score Networks (NCSN) [3]\n",
    "\n",
    "We now consider the approach presented in [3], which consist of using a single network $s_\\theta(x, \\upsilon_t)$ that approaches jointly the score of a sequence of distributions $\\{q_t\\}_{t \\in [\\varepsilon, 1]}$ defined as above but for a sequence of $\\{\\upsilon_t\\}_{t \\in [\\varepsilon, 1]}$. Here, we define $\\upsilon_\\varepsilon=0.02$, $\\upsilon_1 = 10$ and $\\upsilon_t = (\\upsilon_\\varepsilon^{1/\\rho} + \\frac{t - \\varepsilon}{1-\\varepsilon} (\\upsilon_1^{1/\\rho} - \\upsilon_\\varepsilon^{1/\\rho}))^\\rho$ with $\\rho=5$.\n",
    "\n",
    "To do so, we consider the following loss\n",
    "$$ \\operatorname{argmin}_{\\theta}\\mathbb{E}_{t \\in \\mathcal{U}(\\varepsilon, 1)}\\left[\\gamma_t^2\\mathbb{E}_{X \\sim q_d, \\epsilon \\sim \\mathcal{N}(0, \\operatorname{I})}[\\|s_{\\theta}(X + \\upsilon_t \\epsilon) - \\upsilon_t^{-1}\\epsilon \\|^2]\\right]\\, ,$$\n",
    "where $\\gamma_t = \\upsilon_t$ is a weighting coefficient.\n",
    "The goal of this sequence of distributions is to allow for efficient sequential ULA steps, since the distance between two consecutive distributions $q_{t_1}$ and $q_{t_2}$ is close for $t_1 \\approx t_2$, thus using samples from $q_{t_2}$ to initialize ULA for $q_{t_1}$ would be a good initialization. Furthemore, this allows to approximate better $p$ by choosing $\\varepsilon$ small.\n",
    "\n",
    "We start by visualizing the sequence of distributions $\\{q_t\\}_{t \\in [\\varepsilon, 1]}$ defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91249a99935f285",
   "metadata": {
    "id": "a91249a99935f285",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "KEY, subkey = random.split(KEY, 2)\n",
    "std_min = 0.02\n",
    "std_max = 10\n",
    "p=5\n",
    "stds = (std_max ** (1/p) + jnp.linspace(1, 0, 100)*(std_min**(1/p) - std_max**(1/p)))**p #taken from Karras2022 (https://arxiv.org/pdf/2206.00364.pdf)\n",
    "samples = vmap(lambda std_t, key: p_t(std_t).sample(key, sample_shape=(1_000,)))(stds, random.split(subkey, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b61b450ed1c11",
   "metadata": {
    "id": "bb7b61b450ed1c11",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "def animate(t):\n",
    "    plt.cla()\n",
    "    plt.scatter(*samples[t].T)\n",
    "    plt.xlim(-15, 15)\n",
    "    plt.ylim(-15, 15)\n",
    "\n",
    "matplotlib.animation.FuncAnimation(fig, animate, frames=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925e6d1-e8cb-44d8-91de-63df6521b7bf",
   "metadata": {
    "id": "2925e6d1-e8cb-44d8-91de-63df6521b7bf",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def make_ncns_loss(net, std_min=1e-2, std_max=10, data_dim=2, n_time_samples=25):\n",
    "    def loss_fn(params, data, key):\n",
    "        key_uniform, key_noise = random.split(key, 2)\n",
    "        #stds = random.uniform(key_uniform, minval=std_min, maxval=std_max, shape=(data.shape[0], n_time_samples, 1))\n",
    "        stds = jnp.exp(random.normal(key_uniform, shape=(data.shape[0], n_time_samples, 1))*1.2 - .5).clip(std_min, std_max)\n",
    "        eps = random.normal(key, (data.shape[0], n_time_samples, 2))\n",
    "        x_t = data[:, None] + stds * eps\n",
    "        score = net.apply(params, x_t, stds)\n",
    "\n",
    "        loss = ((score + eps/stds[None])**2).sum(axis=-1)\n",
    "        loss_weights = stds[..., 0]**2\n",
    "        return  (loss_weights*loss).mean()\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96fcddd-a41a-4c72-94ff-4687b27a7188",
   "metadata": {
    "id": "c96fcddd-a41a-4c72-94ff-4687b27a7188",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):                    \n",
    "  out_dims: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, std):\n",
    "    std_emb = .25 * jnp.log(std)\n",
    "    x = nn.Dense(256)(jnp.concatenate((x, std_emb), axis=-1))\n",
    "    x = nn.LayerNorm()(x)              \n",
    "    x = nn.leaky_relu(x)\n",
    "    x = nn.Dense(512)(x)\n",
    "    x = nn.leaky_relu(x)\n",
    "    x = nn.Dense(256)(x)   \n",
    "    x = nn.LayerNorm()(x)               \n",
    "    x = nn.leaky_relu(x)\n",
    "    x = nn.Dense(self.out_dims)(x)       \n",
    "    return nn.leaky_relu(x)\n",
    "\n",
    "model = MLP(out_dims=2)\n",
    "x, std = jnp.empty((1, 2)), jnp.empty((1, 1))\n",
    "net_params = model.init(random.key(42), x, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626786d-3ae0-4960-96c8-e9c7e0a29993",
   "metadata": {
    "id": "0626786d-3ae0-4960-96c8-e9c7e0a29993",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "KEY, subkey = random.split(KEY, 2)\n",
    "net_params, losses = train(\n",
    "    dataset=dataset,\n",
    "    learning_rate=1e-2,\n",
    "    net_params=net_params,\n",
    "    loss_fn=value_and_grad(make_ncns_loss(model, std_min=std_min, std_max=std_max, n_time_samples=2)),\n",
    "    key=subkey,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    update_tqdm_period=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656dc46-6f8c-40d0-81a9-cc444b2ba096",
   "metadata": {
    "id": "c656dc46-6f8c-40d0-81a9-cc444b2ba096",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(jnp.stack(losses))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef92696-dfbb-4cad-b0e0-ef70b87cbdca",
   "metadata": {
    "id": "bef92696-dfbb-4cad-b0e0-ef70b87cbdca"
   },
   "source": [
    "## Sampling with NCSN\n",
    "\n",
    "We now consider doing sequentially ULA for each $q_t$, starting from $q_{100}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda48a4-a15a-470c-9c6a-12787d5f6ce9",
   "metadata": {
    "id": "7dda48a4-a15a-470c-9c6a-12787d5f6ce9",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def sequential_ula(\n",
    "        initial_samples,\n",
    "        key,\n",
    "        stds_sequence,\n",
    "        n_iter_per_timestep,\n",
    "        score_fun,\n",
    "        learning_rate_ratio,\n",
    "        n_chains,\n",
    "):\n",
    "    def scan_fn(x, meta):\n",
    "        key, lr, std = meta\n",
    "        samples = multiple_chain_ula(x,\n",
    "                                     random.split(key, n_chains),\n",
    "                                     learning_rate=lr,\n",
    "                                     score_fun=Partial(score_fun, std=std[None]),\n",
    "                                     n_steps=n_iter_per_timestep)\n",
    "        return samples[:, -1], samples\n",
    "    return scan(f=scan_fn,\n",
    "                init=initial_samples,\n",
    "                xs=(random.split(key, len(stds_sequence)),\n",
    "                    learning_rate_ratio * (stds_sequence / stds_sequence[-1])**2,\n",
    "                    stds_sequence))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352135b4-91f5-4d70-8156-80a7891967f2",
   "metadata": {
    "id": "352135b4-91f5-4d70-8156-80a7891967f2",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "KEY, subkey_init = random.split(KEY, 2)\n",
    "samples_ncnn = sequential_ula(\n",
    "    random.normal(subkey_init, shape=(20, 2))*std_max,\n",
    "    subkey_init,\n",
    "    stds_sequence=stds[::-1],\n",
    "    n_iter_per_timestep=5,\n",
    "    score_fun=jit(lambda x, std: model.apply(net_params, x, std)),\n",
    "    learning_rate_ratio=1e-4,\n",
    "    n_chains=20\n",
    ")\n",
    "score_quivers = []\n",
    "for std in stds[::-1]:\n",
    "    X, Y, scores = get_mesh_score(lambda xs: vmap(model.apply, in_axes=(None, 0, None))(net_params, xs, std[None]))\n",
    "    score_quivers.append(scores / jnp.abs(scores).max(axis=0)[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f23691-623a-4714-8766-417ed85cef24",
   "metadata": {
    "id": "c7f23691-623a-4714-8766-417ed85cef24",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.scatter(*dataset[:1000].T, color='blue', alpha=.6)\n",
    "    ax.quiver(X, Y, scores[:, 0], scores[:, 1], color='black')\n",
    "    ax.set_xlim(-15, 15)\n",
    "    ax.set_ylim(-15, 15)\n",
    "\n",
    "traj_artists, points_artists, quiver_artists = [], [], []\n",
    "for ax in axes:\n",
    "    quiver_artists.append(ax.quiver(X, Y, score_quivers[0][:, 0], score_quivers[0][:, 1], color='black'))\n",
    "for i, traj in enumerate(jnp.swapaxes(samples_ncnn[:1], 0, 1)):\n",
    "    point_artist = axes[1].scatter(*traj[-1, -1], color=plt.get_cmap(\"rainbow\")(i / (samples_ncnn.shape[1]-1)), alpha=.8)\n",
    "    traj_artist, = axes[0].plot(*traj.reshape(-1, 2).T, color=plt.get_cmap(\"rainbow\")(i / (samples_ncnn.shape[1]-1)), alpha=.5)\n",
    "    traj_artists.append(traj_artist)\n",
    "    points_artists.append(point_artist)\n",
    "artists = traj_artists + points_artists + quiver_artists\n",
    "\n",
    "def init():\n",
    "\n",
    "  return artists\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    n_artists = (len(artists) - 2) //2\n",
    "    for i, traj in enumerate(jnp.swapaxes(samples_ncnn[max(frame-40, 0):frame + 1], 0, 1)):\n",
    "        artists[i].set_data(traj[..., 0].reshape(-1), traj[..., 1].reshape(-1))\n",
    "        artists[i + n_artists].set_offsets(traj[-1, -1])\n",
    "    artists[-2].set_UVC(score_quivers[frame][:, 0], score_quivers[frame][:, 1])\n",
    "    artists[-1].set_UVC(score_quivers[frame][:, 0], score_quivers[frame][:, 1])\n",
    "    return artists\n",
    "\n",
    "matplotlib.animation.FuncAnimation(fig,  update, frames=jnp.arange(0, 100, 2).tolist(),\n",
    "                    init_func=init, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43fee3-a79f-4304-819e-babb383f08e3",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## DDIM Sampling\n",
    "\n",
    "Note that the sequence of distributions defined above match the marginals of the following Markov chain:\n",
    "\n",
    "$$ X_t = X_{t-1} + (\\upsilon_t^2 - \\upsilon_{t-1}^2)^{1/2} \\epsilon_t \\,,$$\n",
    "\n",
    "where $\\epsilon_t \\sim \\mathcal{N}(0, \\operatorname{I})$ for $X_0 \\sim q_{d}$. We denote the Law of $X_t$ knowing $X_{t-1}$ as $q_{t|t-1}(x_t|x_{t-1}) = \\mathcal{N}(x_t; x_{t-1}, (\\upsilon_t^2 - \\upsilon_{t-1}^2) \\operatorname{I})$.\n",
    "\n",
    "### Inference distribution\n",
    "\n",
    "We now focus on the DDIM sampler [4]. The goal of DDIM is to propose a backward Markov chain that matches well the Markov chain defined above. To do so, it focus first on the Law of $X_{1:T}$ knowing $X_{0}$.\n",
    "It relies on the inference distribution\n",
    "\n",
    "$$ q_{1:T}^{\\eta}(x_{1:T} | x_0) = q_{T|0} \\prod_{i=2}^{T} q_{t-1 | t, 0}^{\\eta_{t-1}}(x_{t-1}|x_t, x_0)\\,,$$ \n",
    "\n",
    "where $\\eta \\in [0, \\infty) \\times (0, \\upsilon_1) \\times \\cdots \\times (0, \\upsilon_{T-1})$ and \n",
    "\n",
    "$$  q_{t-1 | t, 0}^{\\eta_{t-1}}(x_{t-1}|x_t, x_0) = \\mathcal{N}(x_0 + [(\\upsilon_{t-1}^2 - \\eta_{t-1}^2)^{1/2} / \\upsilon_{t}] (x_t - x_0), \\eta_{t-1}^2 \\operatorname{I}) \\,. $$\n",
    "\n",
    "The key property of the inference distribution is that it matches the laws of $X_t$ knowing $X_0$:\n",
    "\n",
    "$$ q_{t|0} = \\int q_{1:T}^{\\eta}(x_{1:T} | x_0) dx_{1:t-1} dx_{t+1:T} = \\mathcal{N}(x_0, \\upsilon_{t}^2 \\operatorname{I}) \\,.$$\n",
    "\n",
    "Indeed, for the *particular choice* of $\\eta_t = (\\upsilon_t^2 - \\upsilon_{t-1}^2)^{1/2} (\\upsilon_{t-1} / \\upsilon_t)$ we retrieve the Bayes decomposition:\n",
    "\n",
    "$$ q_{t-1 | t, 0}^{\\eta_t} = \\frac{q_{t|t-1}(x_t | x_{t-1}) q_{t-1|0}(x_{t-1}|x_{0})}{q_{t|0}(x_t|x_0)}$$\n",
    "\n",
    "[4] Song, J., Meng, C., & Ermon, S. (2020, October). Denoising Diffusion Implicit Models. In International Conference on Learning Representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5b3cd-0416-4999-9cd4-1186dafeb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_process(x_0, x_t, sigma_t, sigma_t_1, eta_t_1):\n",
    "    eps = x_t - x_0\n",
    "    coeff_eps = ((sigma_t_1**2 - eta_t_1**2) / (sigma_t**2))**.5\n",
    "    return x_0 + coeff_eps * eps\n",
    "\n",
    "\n",
    "def inference_process_sampling(x_T, key, x_0, etas, sigmas):\n",
    "    def _infproc_step(x_t, meta):\n",
    "        key, eta_t_1, sig_t, sig_t_1 = meta\n",
    "        x_t_1 = inference_process(x_0, x_t, sig_t, sig_t_1, eta_t_1)\n",
    "        x_t_1 = x_t_1 + eta_t_1 * random.normal(key, shape=x_t_1.shape)\n",
    "        return x_t_1, x_t_1\n",
    "    n_steps = len(etas)\n",
    "    return scan(f=_infproc_step,\n",
    "                init=x_T,\n",
    "                xs=(random.split(key, n_steps), etas[::-1], sigmas[1:][::-1], sigmas[:-1][::-1]))[-1]\n",
    "\n",
    "\n",
    "def make_inference_process_sampler(N, eps, p=5, std_min=0.02, std_max=10.):\n",
    "    stds = (std_max ** (1/p) + jnp.linspace(1, 0, N)*(std_min**(1/p) - std_max**(1/p)))**p\n",
    "    etas = ((stds[1:] ** 2 - stds[:-1] **2)**.5) * (stds[:-1] / stds[1:]) * eps\n",
    "    return jit(vmap(Partial(inference_process_sampling,\n",
    "                            etas=etas,\n",
    "                            sigmas=stds), in_axes=(0, 0, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda4212-68fd-4d54-8fc8-240377f5fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_inf_proc_sampler = make_inference_process_sampler(100, 1)\n",
    "other_inf_proc_sampler = make_inference_process_sampler(100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a1f64-07e0-45db-83d2-4d660d746cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY, subkey_init, subkey_bayes, subkey_2 = random.split(KEY, 4)\n",
    "initial_samples = random.normal(subkey_init, shape=(20, 1))*std_max\n",
    "\n",
    "bayes_samples = bayes_inf_proc_sampler(initial_samples, random.split(subkey_bayes, 20), jnp.zeros((1,)))\n",
    "other_samples = other_inf_proc_sampler(initial_samples, random.split(subkey_2, 20), jnp.zeros((1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6806727-e000-457f-9dbd-6f0795ed3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = other_samples.shape[1]\n",
    "range_to_plot = jnp.linspace(1, 0, n_steps)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-10, 10)\n",
    "ax.set_yscale('symlog')\n",
    "ax.fill_between(range_to_plot[::-1], -stds[:-1]*3, stds[:-1]*3, color='red', alpha=.3)\n",
    "\n",
    "artists = [\n",
    "    ax.plot([], [],  linestyle='dashed', color='blue', marker='o', markersize=3)[0], \n",
    "    ax.plot([], [],  linestyle='dashed', color='green', marker='o', markersize=3)[0]\n",
    "]\n",
    "\n",
    "def init():\n",
    "\n",
    "  return artists\n",
    "\n",
    "def update(t):\n",
    "\n",
    "    artists[0].set_data(range_to_plot[:t+1], bayes_samples[0, :t+1])\n",
    "    artists[1].set_data(range_to_plot[:t+1], other_samples[0, :t+1])\n",
    "    return artists\n",
    "\n",
    "matplotlib.animation.FuncAnimation(\n",
    "    fig,\n",
    "    update,\n",
    "    frames=jnp.arange(0, 100, 1).tolist(),\n",
    "    init_func=init, blit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35ec96-5545-4112-9a36-e4c0e1d8cf3f",
   "metadata": {},
   "source": [
    "### DDIM sampling\n",
    "\n",
    "Then, to generate the backward chain from DDIM, one simply replace in every $q_{t-1|t, 0}$ the $X_0$ term by Tweedie's approximation of the mean obtained with the score net:\n",
    "\n",
    "$$ \\mu_{t, \\theta}(x_t) = x_t + \\upsilon_t^2 s_{\\theta}(x_t, \\upsilon_t)\\,. $$\n",
    "\n",
    "By replacing $q_{T|0}$ by $\\lambda = \\mathcal{N}(0, \\upsilon_T^2 \\operatorname{I})$ we obtain\n",
    "\n",
    "$$p_{0:T}(x_{0:T}) = \\lambda(x_T) \\prod_{t=1}^{T} p_{t-1|t}(x_{t-1}| x_t) \\,,$$\n",
    "\n",
    "where $p_{t-1|t} = q_{t-1|t, 0}(x_{t-1}|x_{t}, \\mu_{t, \\theta}(x_t))$ for $t > 1$ and $p_{0|1} = \\mathcal{N}(\\mu_{1, \\theta}(x_1), \\eta_0^2 \\operatorname{I})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1014a8-8cb2-474c-9e99-4b91f7eff3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddim_sampling(x_T, key, etas, sigmas, score_net):\n",
    "    \n",
    "    def _ddim_step(x_t, meta):\n",
    "        key, eta_t_1, sig_t, sig_t_1 = meta\n",
    "        pred_x_0 = x_t + (sig_t**2) * score_net(x_t, sig_t)\n",
    "        x_t_1 = inference_process(pred_x_0, x_t, sig_t, sig_t_1, eta_t_1)\n",
    "        x_t_1 = x_t_1 + eta_t_1 * random.normal(key, shape=x_t_1.shape)\n",
    "        return x_t_1, x_t_1\n",
    "        \n",
    "    n_steps = len(etas) - 1\n",
    "    x_1, x_traj = scan(f=_ddim_step,\n",
    "                init=x_T,\n",
    "                xs=(random.split(key, n_steps), etas[1:][::-1], sigmas[1:][::-1], sigmas[:-1][::-1]))\n",
    "    x_0 = x_1 + (sigmas[0]**2) * score_net(x_1, sigmas[0])\n",
    "    x_0 = x_0 + etas[0] * random.normal(key, shape=x_0.shape) \n",
    "    return jnp.concatenate((x_traj, x_0[None]), axis=0)\n",
    "\n",
    "\n",
    "def make_ddim_sampler(N, eps, score_net, eta_0=0., p=5, std_min=0.02, std_max=10.):\n",
    "    stds = (std_max ** (1/p) + jnp.linspace(1, 0, N)*(std_min**(1/p) - std_max**(1/p)))**p\n",
    "    etas = jnp.clip(((stds[1:] ** 2 - stds[:-1] **2)**.5) * (stds[:-1] / stds[1:]) * eps, 0, stds[:-1] - 1e-8)\n",
    "    etas = jnp.concatenate((jnp.array([eta_0]), etas), axis=0)\n",
    "    return jit(vmap(Partial(ddim_sampling,\n",
    "                            etas=etas,\n",
    "                            sigmas=stds,\n",
    "                           score_net=score_net), in_axes=(0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc52213-3ed8-43b7-8d12-3e71f1703a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_ddim_sampler = make_ddim_sampler(50, 1, score_net=lambda x, std: model.apply(net_params, x, std[None]))\n",
    "other_ddim_sampler = make_ddim_sampler(50, 0.2, score_net=lambda x, std: model.apply(net_params, x, std[None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e52ca4-b18f-4ce0-b3f6-5560db4c6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY, subkey_init, subkey_bayes, subkey_other = random.split(KEY, 4)\n",
    "initial_samples = random.normal(subkey_init, shape=(100, 2))*std_max\n",
    "\n",
    "bayes_samples = bayes_ddim_sampler(initial_samples, random.split(subkey_bayes, 100))\n",
    "other_samples = other_ddim_sampler(initial_samples, random.split(subkey_other, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208f454-797f-4e9d-967b-83e58414f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.scatter(*dataset[:1000].T, color='blue', alpha=.6)\n",
    "    # ax.quiver(X, Y, scores[:, 0], scores[:, 1], color='black')\n",
    "    ax.set_xlim(-15, 15)\n",
    "    ax.set_ylim(-15, 15)\n",
    "axes[0].set_title('DDIM other')\n",
    "axes[1].set_title('DDIM Bayes')\n",
    "other_artists, bayes_artists, quiver_artists = [], [], []\n",
    "# for ax in axes:\n",
    "#     quiver_artists.append(ax.quiver(X, Y, score_quivers[0][:, 0], score_quivers[0][:, 1], color='black'))\n",
    "\n",
    "other_artists.append(axes[0].scatter(*initial_samples.T, color='orange'))\n",
    "bayes_artists.append(axes[1].scatter(*initial_samples.T, color='red'))\n",
    "artists = other_artists + bayes_artists + quiver_artists\n",
    "\n",
    "def init():\n",
    "\n",
    "  return artists\n",
    "\n",
    "\n",
    "def update(t):\n",
    "    f1, f2 = t\n",
    "    artists[0].set_offsets(other_samples[:, f1])\n",
    "    artists[1].set_offsets(bayes_samples[:, f2])\n",
    "    # artists[-2].set_UVC(score_quivers[frame][:, 0], score_quivers[frame][:, 1])\n",
    "    # artists[-1].set_UVC(score_quivers[frame][:, 0], score_quivers[frame][:, 1])\n",
    "    return artists\n",
    "\n",
    "min_len = min(bayes_samples.shape[1], other_samples.shape[1])\n",
    "t2 = jnp.trunc(jnp.linspace(0, bayes_samples.shape[1], min_len)).astype(int)\n",
    "t1 = jnp.trunc(jnp.linspace(0, other_samples.shape[1], min_len)).astype(int)\n",
    "matplotlib.animation.FuncAnimation(fig,  update, frames=zip(t1, t2),\n",
    "                    init_func=init, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03bc7f6-5343-4df9-9fe7-0363adb9b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ot import max_sliced_wasserstein_distance\n",
    "KEY, subkey_init = random.split(KEY, 2)\n",
    "initial_samples = random.normal(subkey_init, shape=(2000, 2))*std_max\n",
    "ddim_sws = {}\n",
    "for eps in [0., 0.1, 0.5, 1.]:\n",
    "    ddim_sws[eps] = {'N': [], 'sw': []}\n",
    "    for N in jnp.arange(2, 11, 1)**2:\n",
    "        KEY, subkey_sampler = random.split(KEY, 2)\n",
    "        samples = make_ddim_sampler(N, eps, score_net=lambda x, std: model.apply(net_params, x, std[None]))(initial_samples, random.split(subkey_sampler, initial_samples.shape[0]))\n",
    "        sw = max_sliced_wasserstein_distance(samples[:,-1], dataset, n_projections=1000)\n",
    "        ddim_sws[eps]['N'].append(N)\n",
    "        ddim_sws[eps]['sw'].append(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8b75d-d8b8-43c8-aeae-264276ecbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "for c, (eps, eps_data) in zip(plt.get_cmap(\"rainbow\")(jnp.arange(len(ddim_sws)) / (len(ddim_sws) - 1)), ddim_sws.items()):\n",
    "    ax.plot(eps_data['N'], eps_data['sw'], color=c, marker='*', linestyle='dashed', label=eps)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Sliced Wasserstein')\n",
    "ax.set_xlabel('N')\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4951a6-e4b0-4653-ac47-9592b4d890e2",
   "metadata": {},
   "source": [
    "# Extra: Celeb dataset and Hugging Faces \n",
    "\n",
    "We now consider the dataset to be the Celeba HQ 256 and we use the pretrained model from Hugging Faces' diffusers library (https://github.com/huggingface/diffusers).\n",
    "\n",
    "The main difference with the previous sections, is that, as in [4], they use the *Variance Preserving* framework, which corresponds to simply changing the perturbation kernel to:\n",
    "\n",
    "$$ q_{t|0}(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_0, (1 - \\alpha_t) \\operatorname{I})\\,,$$\n",
    "\n",
    "where ${\\alpha_t}_{t \\in [\\varepsilon, 1]}$ is such that $\\alpha_t \\in (0, 1)$ and $\\lim_{t\\rightarrow 1} \\alpha_t = 0$. All the calculations above are feasible with this perturbation Kernel, except that the distribution $\\lambda_n = \\mathcal{N}(0, \\operatorname{I})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f1f89-3dca-4013-9bf0-688639f6a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMPipeline\n",
    "\n",
    "model_id = \"google/ddpm-ema-celebahq-256\"\n",
    "\n",
    "pipe = DDIMPipeline.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd8626-57d0-4437-bb79-186cee8c77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(eta=1, num_inference_steps=10).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f13d8b-0b05-413b-b162-c9050825c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(eta=0, num_inference_steps=10).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a775ec-6a3a-4f09-9c44-d9f445a64a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
