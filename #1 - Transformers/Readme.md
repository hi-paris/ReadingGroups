## Edition 1 - Transformers

â¡ï¸ For the 1st edition of the Hi! PARIS Reading groups, we teach you the Transformers.

The edition consists on 3 sessions.
The reading group will present the architecture of Transformers and their applications in Time Series and Generative Models

### Session 1/3 â€“ Introduction to Transformers
Tuesday 19 October, 2021 â€“ 2.00-3.30pm (Online)


**ğŸ“£ Speakers**

* Charles Ollion, Ã‰cole polytechnique 

* Sylvain Le Corff, TÃ©lÃ©com SudParis 


**ğŸ“š Program**

â€“ Introduction to transformers: motivations & current uses (~15min presentation).\
â€“ Typical mathematical models for transformers (~20min).\
â€“ Diving into details: building blocks, important tricks, example code, & visualisation of typical transformers (~40min).


**ğŸ“‘ Papers**

â€“ Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. 31st Conference on Neural Information Processing Systems. CA, USA: Long Beach.\
â€“ Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding\ 
â€“ Li, S.; Jin, X.; Xuan, Y.; Zhou, X.; Chen, W.; Wang, Y.-X.; and Yan, X. 2019. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in Neural Information Processing Systems, 32\


**ğŸ Presentatin/Notebook/simulations**

â€“ see above ou [Collab weblink](https://colab.research.google.com/github/charlesollion/dlexperiments/blob/master/5-Transformers-Intro/TransformersHandsOn.ipynb)


### Session 2/3 â€“ Transformers for Times Series
Tuesday 30 November, 2021 â€“ 2.00-3.30pm (Online)


**ğŸ“£ Speakers**

* Charles Ollion, Ã‰cole polytechnique 

* Sylvain Le Corff, TÃ©lÃ©com SudParis  


**ğŸ“š Program**

â€“ Applications of Transformers networks for time series prediction.\
â€“ Comments on the links with recurrent networks.

**ğŸ“‘ Papers**

â€“ Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. 31st Conference on Neural Information Processing Systems. CA, USA: Long Beach.\
â€“ Omer Levy, Kenton Lee, Nicholas FitzGerald, and Luke Zettlemoyer. Long short-term memory as a dynamically computed element-wise weighted sum. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 732â€“739, 2018.\  


**ğŸ Presentatin/Notebook/simulations**

â€“ see above ou [Collab weblink](https://colab.research.google.com/github/charlesollion/dlexperiments/blob/master/7-Transformers-Timeseries/Transformers_for_timeseries.ipynb)


### Session 3/3 â€“ Generative Models based on Transformers
Tuesday 11 January, 2022 â€“ 2.00-3.30pm (Online)


**ğŸ“£ Speakers**

* Charles Ollion, Ã‰cole polytechnique 

* Sylvain Le Corff, TÃ©lÃ©com SudParis  


**ğŸ“‘ Papers**

â€“ Alice Martin, Charles Ollion, Florian Strub, Sylvain Le Corff, and Olivier Pietquin. The monte carlo transformer: a stochastic self-attention model for sequence prediction. CoRR, abs/2007.08620, 2020\


**ğŸ Presentatin/Notebook/simulations**

â€“ see above