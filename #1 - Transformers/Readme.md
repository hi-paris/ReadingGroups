## Edition 1 - Transformers

➡️ For the 1st edition of the Hi! PARIS Reading groups, we teach you the Transformers.

The edition consists on 3 sessions.
The reading group will present the architecture of Transformers and their applications in Time Series and Generative Models

### Session 1/3 – Introduction to Transformers
Tuesday 19 October, 2021 – 2.00-3.30pm (Online)


**📣 Speakers**

* Charles Ollion, École polytechnique 

* Sylvain Le Corff, Télécom SudParis 


**📚 Program**

– Introduction to transformers: motivations & current uses (~15min presentation).\
– Typical mathematical models for transformers (~20min).\
– Diving into details: building blocks, important tricks, example code, & visualisation of typical transformers (~40min).


**📑 Papers**

– Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. 31st Conference on Neural Information Processing Systems. CA, USA: Long Beach.\
– Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding\ 
– Li, S.; Jin, X.; Xuan, Y.; Zhou, X.; Chen, W.; Wang, Y.-X.; and Yan, X. 2019. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in Neural Information Processing Systems, 32\


**🐍 Presentatin/Notebook/simulations**

– see above ou [Collab weblink](https://colab.research.google.com/github/charlesollion/dlexperiments/blob/master/5-Transformers-Intro/TransformersHandsOn.ipynb)


### Session 2/3 – Transformers for Times Series
Tuesday 30 November, 2021 – 2.00-3.30pm (Online)


**📣 Speakers**

* Charles Ollion, École polytechnique 

* Sylvain Le Corff, Télécom SudParis  


**📚 Program**

– Applications of Transformers networks for time series prediction.\
– Comments on the links with recurrent networks.

**📑 Papers**

– Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. 31st Conference on Neural Information Processing Systems. CA, USA: Long Beach.\
– Omer Levy, Kenton Lee, Nicholas FitzGerald, and Luke Zettlemoyer. Long short-term memory as a dynamically computed element-wise weighted sum. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 732–739, 2018.\  


**🐍 Presentatin/Notebook/simulations**

– see above ou [Collab weblink](https://colab.research.google.com/github/charlesollion/dlexperiments/blob/master/7-Transformers-Timeseries/Transformers_for_timeseries.ipynb)


### Session 3/3 – Generative Models based on Transformers
Tuesday 11 January, 2022 – 2.00-3.30pm (Online)


**📣 Speakers**

* Charles Ollion, École polytechnique 

* Sylvain Le Corff, Télécom SudParis  


**📑 Papers**

– Alice Martin, Charles Ollion, Florian Strub, Sylvain Le Corff, and Olivier Pietquin. The monte carlo transformer: a stochastic self-attention model for sequence prediction. CoRR, abs/2007.08620, 2020\


**🐍 Presentatin/Notebook/simulations**

– see above